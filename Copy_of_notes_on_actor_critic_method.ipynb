{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turbulent0/leetcode_problems/blob/master/Copy_of_notes_on_actor_critic_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor Critic Method Notebook\n",
        "\n",
        "*- Apoorv Nandan*\n",
        "\n",
        "*Intended for people who are familiar with machine learning and neural networks in general.*"
      ],
      "metadata": {
        "id": "rVqP4wee8RFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TLDR Intro to RL\n",
        "So for those of you that are not at all familiar with reinforcement learning, here's a quick gist:\n",
        "\n",
        "- In RL, we don't have datasets as such. We instead have an environment, a simulation e.g. a game like flappy bird or dota.\n",
        "- Then, we train an agent (which is just one or more neural networks), to achieve some sort of an objective within the environment. (like winning the game, or scoring high)\n",
        "\n"
      ],
      "metadata": {
        "id": "6-iHV5k18wk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent and Environment\n",
        "\n",
        "In actor critic method, we have two neural networks inside our agent - an actor and a critic. They can optionaly share parameters as well, as they both operate on the same input - the observations that you get from the environment. These observations are also known as the state of the environment. The environment and the observations are often set up in a way where you can represent all the observations with a tensor, which can be fed into our agent.\n",
        "\n",
        "Let's consider the hello-world parallel of RL environments - cartpole.\n",
        "\n",
        "Cartpole environment has two things:\n",
        "* Cart: A movable cart that can move left or right along a frictionless track.\n",
        "* Pole: A rigid pole attached to the cart by a hinge, allowing it to swing freely in the vertical plane.\n",
        "\n",
        "![cartpole image](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "\n",
        "The observations are just 4 numbers which we combine to create an input tensor of shape `(4,)`.\n",
        "\n",
        "The 4 numbers represent:\n",
        "* Cart Position (x): The horizontal position of the cart on the track.\n",
        "* Cart Velocity (x_dot): The velocity of the cart.\n",
        "* Pole Angle (θ): The angle of the pole with respect to the vertical.\n",
        "* Pole Angular Velocity (θ_dot): The angular velocity of the pole.\n",
        "\n",
        "The actor network will take this as the input, and predict probabilities for each of the possible actions in this env, so that the agent can decide what to do based on the observations.\n",
        "\n",
        "There are just two possible actions here:\n",
        "* 0: Apply a force to the left.\n",
        "* 1: Apply a force to the right.\n",
        "\n",
        "The critic network will take the same input, and predict a single number which we will call the `critic value`. This represents an estimate of the total rewards the agent should be able to get from this point onwards, looking at the current observations from the environment.\n",
        "\n",
        "So now we can make a loop to make the agent play around inside the environment.\n",
        "\n",
        "The loop looks like this:\n",
        "```\n",
        "               ┌───────┐\n",
        "   ┌──────────▶│ Agent │────────────┐\n",
        "   │           └───────┘            │\n",
        "   │        ┌─────────────┐         │\n",
        "   └────────│ Environment │◀────────┘\n",
        "            └─────────────┘\n",
        "```\n",
        "The agent chooses an action to perform (based on the probabilities predicted by the actor network). The environment processes that action, and gives back a new set of observations for the agent to act upon.\n",
        "\n",
        "Let's code this;\n"
      ],
      "metadata": {
        "id": "Kn6KE11R94oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym # contains predefined envs that can process the agent's action,\n",
        "# and return the rewards and the observations.\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "env = gym.make(\"CartPole-v0\")  # create the environment\n",
        "env.seed(42)\n",
        "\n",
        "# actor and critic will be simple feed forward networks, and they will share the first layer.\n",
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "def init_xavier_weights(shape):\n",
        "    return np.random.randn(*shape) * np.sqrt(2 / shape[0])\n",
        "\n",
        "def init_agent_params():\n",
        "    w_common = init_xavier_weights((num_inputs, num_hidden))\n",
        "    w_actor = init_xavier_weights((num_hidden, num_actions))\n",
        "    w_critic = init_xavier_weights((num_hidden, 1))\n",
        "    return {\n",
        "        'w_common': w_common,\n",
        "        'w_actor': w_actor,\n",
        "        'w_critic': w_critic\n",
        "    }\n",
        "\n",
        "agent_params = init_agent_params()\n",
        "\n",
        "max_steps_per_episode = 10000\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # forward pass of agent to get action probs and critic value\n",
        "        h = np.dot(state, agent_params['w_common'])\n",
        "        h = np.maximum(h, 0)\n",
        "        logits = np.dot(h, agent_params['w_actor'])\n",
        "        action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "\n",
        "        critic_value = np.dot(h, agent_params['w_critic'])\n",
        "        critic_value_history.append(critic_value)\n",
        "\n",
        "        chosen_action = np.random.choice(num_actions, p=action_probs)\n",
        "        chosen_action_prob = action_probs[chosen_action]\n",
        "        logprob = np.log(chosen_action_prob)\n",
        "        action_probs_history.append(np.log(action_probs[chosen_action]))\n",
        "\n",
        "        state, reward, done, _ = env.step(chosen_action)\n",
        "        rewards_history.append(reward)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # running reward will be used to determine if we should stop training\n",
        "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "    # TODO: train the actor and critic networks so that they do better in\n",
        "    # the next iteration\n",
        "\n",
        "    episode_count += 1\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n",
        "\n",
        "    break # JUST FOR DEMO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vE5CH5kq8Jm0",
        "outputId": "22fd0ee0-9a79-4eea-9cf9-e58de8312e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code runs the loop in what we call episodes. An episode is just a collection of timesteps. It can go up till `max_steps_per_episode` or it can end early if the agent ends up going to a state where there is nothing left to do.\n",
        "\n",
        "In cartpole env, an episode ends when the pole falls beyond a certain angle, the cart moves too far from the center, or the maximum number of time steps is reached.\n",
        "\n",
        "During the episode, we keep track of the actions chosen by the agent, the log probabilities of the chosen action, the critic estimates, and the rewards given back by the env after processing the action.\n",
        "\n",
        "The agent receives a reward of +1 for each time step that the pole remains upright.\n",
        "\n",
        "After each episode, we tune the parameters towards a point where the agent actually performs well within the environment. To do that, we need to calculate some loss values for the actor and critic, and then calculate gradients using that so that the loss values can be minimised."
      ],
      "metadata": {
        "id": "8UzpjjEKGwB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Actor and Critic\n",
        "Okay, now we need to calculate the loss values for actor and critic.\n",
        "\n",
        "I will write heavily commented code below to help you understand what's happening. Note that we are using the tape everywhere to keep track of everything needed to calculate the gradients afterwards, as we do these calculations."
      ],
      "metadata": {
        "id": "KUVYIFlJlFIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf5F0pRr8Hgl",
        "outputId": "a038aa9c-1540-4aef-9d78-a5df9eb045f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 27.884028067434656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import gym # contains predefined envs that can process the agent's action, and return the rewards and the observations.\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(42)\n",
        "\n",
        "# actor and critic will be simple feed forward networks, and they will share the first layer.\n",
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "def init_xavier_weights(shape):\n",
        "    return np.random.randn(*shape) * np.sqrt(2 / shape[0])\n",
        "\n",
        "def init_agent_params():\n",
        "    w_common = init_xavier_weights((num_inputs, num_hidden))\n",
        "    w_actor = init_xavier_weights((num_hidden, num_actions))\n",
        "    w_critic = init_xavier_weights((num_hidden, 1))\n",
        "    return {\n",
        "        'w_common': w_common,\n",
        "        'w_actor': w_actor,\n",
        "        'w_critic': w_critic\n",
        "    }\n",
        "\n",
        "agent_params = init_agent_params()\n",
        "\n",
        "max_steps_per_episode = 10000\n",
        "gamma = 0.99\n",
        "eps = np.finfo(np.float32).eps.item() # for softmax\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "# will be used in the calculation of critic loss\n",
        "def huber_loss_single(y_true, y_pred, delta=1.0):\n",
        "    # y_true and y_pred are just numbers, not arrays\n",
        "    error = y_true - y_pred\n",
        "    abs_error = np.abs(error)\n",
        "    if abs_error <= delta:\n",
        "        return 0.5 * error**2\n",
        "    else:\n",
        "        return delta * (abs_error - 0.5 * delta)\n",
        "\n",
        "ctx = {\n",
        "    'obs': [],\n",
        "    'common_out': [], # each item in list corresponds to one timestep\n",
        "    'relu_out': [],\n",
        "    'actor_logits': [],\n",
        "    'action_probs': [],\n",
        "    'chosen_action': [],\n",
        "    'chosen_action_prob': [],\n",
        "    'chosen_action_logprob': [],\n",
        "    'critic_value': [],\n",
        "    'return': [],\n",
        "    'ret-val': [],\n",
        "}\n",
        "\n",
        "while True:\n",
        "    observations = env.reset()\n",
        "    episode_reward = 0\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # forward pass of agent to get action probs and critic value\n",
        "        obs = observations.reshape(1, -1)\n",
        "        ctx['obs'].append(obs)\n",
        "        h = np.dot(obs, agent_params['w_common'])\n",
        "        ctx['common_out'].append(h)\n",
        "        h = np.maximum(h, 0)\n",
        "        ctx['relu_out'].append(h)\n",
        "        logits = np.dot(h, agent_params['w_actor'])\n",
        "        ctx['actor_logits'].append(logits)\n",
        "        action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        ctx['action_probs'].append(action_probs)\n",
        "\n",
        "        critic_value = np.dot(h, agent_params['w_critic'])[0,0]\n",
        "        ctx['critic_value'].append(critic_value)\n",
        "        critic_value_history.append(critic_value)\n",
        "\n",
        "        chosen_action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "        ctx['chosen_action'].append(chosen_action)\n",
        "        chosen_action_prob = action_probs[0, chosen_action]\n",
        "        ctx['chosen_action_prob'].append(chosen_action_prob)\n",
        "        logprob = np.log(chosen_action_prob)\n",
        "        ctx['chosen_action_logprob'].append(logprob)\n",
        "        action_probs_history.append(logprob)\n",
        "\n",
        "        observations, reward, done, _ = env.step(chosen_action)\n",
        "        rewards_history.append(reward)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # running reward will be used to determine if we should stop training\n",
        "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "    # train the actor and critic networks so that they do better in\n",
        "    # the next iteration\n",
        "\n",
        "    # Calculate expected value from rewards\n",
        "    # - At each timestep what was the total reward received after that timestep\n",
        "    # - Rewards in the past are discounted by multiplying them with gamma\n",
        "    # - These are the labels for our critic - what our critic is expected\n",
        "    #   to predict accurately.\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "        discounted_sum = r + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "\n",
        "    # Normalize\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    history = zip(action_probs_history, critic_value_history, returns)\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    for log_prob, value, ret in history:\n",
        "        # At this point in history, the critic estimated that we would get a\n",
        "        # total reward = `value` in the future. We took an action with log probability\n",
        "        # of `log_prob` and ended up receiving a total reward = `ret`.\n",
        "        # The actor must be updated so that it predicts an action that leads to\n",
        "        # high rewards (compared to critic's estimate) with high probability.\n",
        "        ctx['return'].append(ret)\n",
        "        diff = ret - value\n",
        "        ctx['ret-val'].append(diff)\n",
        "        actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "        # The critic must be updated so that it predicts a better estimate of\n",
        "        # the future rewards.\n",
        "        l = huber_loss_single(ret, value)\n",
        "        critic_losses.append(l)\n",
        "\n",
        "    loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "    print('loss:', loss_value)\n",
        "    break # just to test\n",
        "\n",
        "    episode_count += 1\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Gradients\n",
        "\n",
        "Okay now we need to calculate the gradients using chain rule all the way from the loss value till the agent parameters. We have all the values we need stored inside `ctx`. We will go backward one operation at a time and accumulate gradients for each intermiediary tensor, till we reach the agent parameters."
      ],
      "metadata": {
        "id": "xs3T45St8yoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradients(ctx, agent_params):\n",
        "    grads = {\n",
        "        'common_out': [], # each item in list corresponds to one timestep\n",
        "        'relu_out': [],\n",
        "        'actor_logits': [],\n",
        "        'action_probs': [],\n",
        "        'chosen_action_prob': [],\n",
        "        'chosen_action_logprob': [],\n",
        "        'critic_value': [],\n",
        "        'ret-val': [],\n",
        "    }\n",
        "\n",
        "    agent_param_grads = {}\n",
        "    # initialise with zero gradients using np.zeros_like\n",
        "    for key in grads:\n",
        "        for item in ctx[key]:\n",
        "            grads[key].append(np.zeros_like(item))\n",
        "\n",
        "    for key in agent_params:\n",
        "        agent_param_grads[key] = np.zeros_like(agent_params[key])\n",
        "\n",
        "    # we just go backward one operation at a time\n",
        "    for step in range(len(ctx['common_out'])):\n",
        "        # backward for -log_prob * diff\n",
        "        grads['chosen_action_logprob'][step] += -1 * ctx['ret-val'][step]\n",
        "        grads['ret-val'][step] += -1 * ctx['chosen_action_logprob'][step]\n",
        "\n",
        "        # backward for huber loss\n",
        "        err = ctx['return'][step] - ctx['critic_value'][step]\n",
        "        if np.abs(err) <= 1.0:\n",
        "            grads['critic_value'][step] += -1 * err\n",
        "        else:\n",
        "            grads['critic_value'][step] += -1 * np.sign(err)\n",
        "\n",
        "        # backward for diff = ret - value\n",
        "        grads['critic_value'][step] += -1 * grads['ret-val'][step]\n",
        "\n",
        "        # backward for np.dot(h, agent_params['w_critic'])\n",
        "        # h is (1, 128) and w_critic is (128, 1)\n",
        "        grads['relu_out'][step] += np.dot(grads['critic_value'][step], agent_params['w_critic'].T)\n",
        "        agent_param_grads['w_critic'] += np.dot(ctx['relu_out'][step].T, grads['critic_value'][step])\n",
        "\n",
        "        # backward for logprob = np.log(chosen_action_prob)\n",
        "        grads['chosen_action_prob'][step] += grads['chosen_action_logprob'][step] / ctx['chosen_action_prob'][step]\n",
        "\n",
        "        # backward for chosing an action\n",
        "        grads['action_probs'][step][0, ctx['chosen_action'][step]] += grads['chosen_action_prob'][step]\n",
        "\n",
        "        # backward for softmax\n",
        "        sum_term = np.sum(grads['action_probs'][step] * ctx['action_probs'][step])\n",
        "        grads['actor_logits'][step] += ctx['action_probs'][step] * (grads['action_probs'][step] - sum_term)\n",
        "\n",
        "        # backward for np.dot(relu_out, agent_params['w_actor'])\n",
        "        # relu_out is (1, 128) and w_actor is (128, 2)\n",
        "        grads['relu_out'][step] += np.dot(grads['actor_logits'][step], agent_params['w_actor'].T)\n",
        "        agent_param_grads['w_actor'] += np.dot(ctx['relu_out'][step].T, grads['actor_logits'][step])\n",
        "\n",
        "        # backward for relu\n",
        "        grads['common_out'][step] += grads['relu_out'][step] * (ctx['common_out'][step] > 0)\n",
        "\n",
        "        # backward for np.dot(obs, agent_params['w_common'])\n",
        "        agent_param_grads['w_common'] += np.dot(ctx['obs'][step].T, grads['common_out'][step])\n",
        "\n",
        "    return agent_param_grads\n",
        "\n",
        "\n",
        "grads = calculate_gradients(ctx, agent_params)\n",
        "for key in grads:\n",
        "    print(key, grads[key].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7gALnFz2dWA",
        "outputId": "9ba9e05d-7e87-4f92-a555-82b2eedfeb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_common (4, 128)\n",
            "w_actor (128, 2)\n",
            "w_critic (128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimisation Step\n",
        "\n",
        "Now that we have the gradients, we can modify the parameters using a learning rate and the gradients and see if the agent learns over time."
      ],
      "metadata": {
        "id": "Pw9RN4Mrva4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym # contains predefined envs that can process the agent's action, and return the rewards and the observations.\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(42)\n",
        "\n",
        "# actor and critic will be simple feed forward networks, and they will share the first layer.\n",
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "def init_xavier_weights(shape):\n",
        "    return np.random.randn(*shape) * np.sqrt(2 / shape[0])\n",
        "\n",
        "def init_agent_params():\n",
        "    w_common = init_xavier_weights((num_inputs, num_hidden))\n",
        "    w_actor = init_xavier_weights((num_hidden, num_actions))\n",
        "    w_critic = init_xavier_weights((num_hidden, 1))\n",
        "    return {\n",
        "        'w_common': w_common,\n",
        "        'w_actor': w_actor,\n",
        "        'w_critic': w_critic\n",
        "    }\n",
        "\n",
        "agent_params = init_agent_params()\n",
        "\n",
        "max_steps_per_episode = 10000\n",
        "gamma = 0.99\n",
        "eps = np.finfo(np.float32).eps.item() # for softmax\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "# will be used in the calculation of critic loss\n",
        "def huber_loss_single(y_true, y_pred, delta=1.0):\n",
        "    # y_true and y_pred are just numbers, not arrays\n",
        "    error = y_true - y_pred\n",
        "    abs_error = np.abs(error)\n",
        "    if abs_error <= delta:\n",
        "        return 0.5 * error**2\n",
        "    else:\n",
        "        return delta * (abs_error - 0.5 * delta)\n",
        "\n",
        "\n",
        "while True:\n",
        "    observations = env.reset()\n",
        "    episode_reward = 0\n",
        "    ctx = {\n",
        "        'obs': [],\n",
        "        'common_out': [], # each item in list corresponds to one timestep\n",
        "        'relu_out': [],\n",
        "        'actor_logits': [],\n",
        "        'action_probs': [],\n",
        "        'chosen_action': [],\n",
        "        'chosen_action_prob': [],\n",
        "        'chosen_action_logprob': [],\n",
        "        'critic_value': [],\n",
        "        'return': [],\n",
        "        'ret-val': [],\n",
        "    }\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # forward pass of agent to get action probs and critic value\n",
        "        obs = observations.reshape(1, -1)\n",
        "        ctx['obs'].append(obs)\n",
        "        h = np.dot(obs, agent_params['w_common'])\n",
        "        ctx['common_out'].append(h)\n",
        "        h = np.maximum(h, 0)\n",
        "        ctx['relu_out'].append(h)\n",
        "        logits = np.dot(h, agent_params['w_actor'])\n",
        "        ctx['actor_logits'].append(logits)\n",
        "        action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        ctx['action_probs'].append(action_probs)\n",
        "\n",
        "        critic_value = np.dot(h, agent_params['w_critic'])[0,0]\n",
        "        ctx['critic_value'].append(critic_value)\n",
        "        critic_value_history.append(critic_value)\n",
        "\n",
        "        chosen_action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "        ctx['chosen_action'].append(chosen_action)\n",
        "        chosen_action_prob = action_probs[0, chosen_action]\n",
        "        ctx['chosen_action_prob'].append(chosen_action_prob)\n",
        "        logprob = np.log(chosen_action_prob)\n",
        "        ctx['chosen_action_logprob'].append(logprob)\n",
        "        action_probs_history.append(logprob)\n",
        "\n",
        "        observations, reward, done, _ = env.step(chosen_action)\n",
        "        rewards_history.append(reward)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # running reward will be used to determine if we should stop training\n",
        "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "    # train the actor and critic networks so that they do better in\n",
        "    # the next iteration\n",
        "\n",
        "    # Calculate expected value from rewards\n",
        "    # - At each timestep what was the total reward received after that timestep\n",
        "    # - Rewards in the past are discounted by multiplying them with gamma\n",
        "    # - These are the labels for our critic - what our critic is expected\n",
        "    #   to predict accurately.\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "        discounted_sum = r + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "\n",
        "    # Normalize\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    history = zip(action_probs_history, critic_value_history, returns)\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    for log_prob, value, ret in history:\n",
        "        # At this point in history, the critic estimated that we would get a\n",
        "        # total reward = `value` in the future. We took an action with log probability\n",
        "        # of `log_prob` and ended up receiving a total reward = `ret`.\n",
        "        # The actor must be updated so that it predicts an action that leads to\n",
        "        # high rewards (compared to critic's estimate) with high probability.\n",
        "        ctx['return'].append(ret)\n",
        "        diff = ret - value\n",
        "        ctx['ret-val'].append(diff)\n",
        "        actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "        # The critic must be updated so that it predicts a better estimate of\n",
        "        # the future rewards.\n",
        "        l = huber_loss_single(ret, value)\n",
        "        critic_losses.append(l)\n",
        "\n",
        "    loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "\n",
        "    grads = calculate_gradients(ctx, agent_params)\n",
        "    # update params\n",
        "    lr = 0.005\n",
        "    for key in agent_params:\n",
        "        agent_params[key] -= lr * grads[key]\n",
        "\n",
        "    action_probs_history.clear()\n",
        "    critic_value_history.clear()\n",
        "    rewards_history.clear()\n",
        "\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCnAeFsz2ehX",
        "outputId": "1a64abb5-09ea-4d59-9443-65eda2ec1ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 13.63 at episode 10\n",
            "running reward: 17.57 at episode 20\n",
            "running reward: 16.45 at episode 30\n",
            "running reward: 14.06 at episode 40\n",
            "running reward: 13.27 at episode 50\n",
            "running reward: 12.35 at episode 60\n",
            "running reward: 12.22 at episode 70\n",
            "running reward: 11.97 at episode 80\n",
            "running reward: 12.28 at episode 90\n",
            "running reward: 11.88 at episode 100\n",
            "running reward: 12.29 at episode 110\n",
            "running reward: 11.75 at episode 120\n",
            "running reward: 12.35 at episode 130\n",
            "running reward: 12.85 at episode 140\n",
            "running reward: 12.50 at episode 150\n",
            "running reward: 12.96 at episode 160\n",
            "running reward: 12.30 at episode 170\n",
            "running reward: 11.65 at episode 180\n",
            "running reward: 13.00 at episode 190\n",
            "running reward: 14.14 at episode 200\n",
            "running reward: 13.92 at episode 210\n",
            "running reward: 12.59 at episode 220\n",
            "running reward: 12.70 at episode 230\n",
            "running reward: 12.54 at episode 240\n",
            "running reward: 13.69 at episode 250\n",
            "running reward: 13.64 at episode 260\n",
            "running reward: 13.35 at episode 270\n",
            "running reward: 14.05 at episode 280\n",
            "running reward: 12.25 at episode 290\n",
            "running reward: 12.08 at episode 300\n",
            "running reward: 13.32 at episode 310\n",
            "running reward: 13.55 at episode 320\n",
            "running reward: 14.97 at episode 330\n",
            "running reward: 14.89 at episode 340\n",
            "running reward: 14.44 at episode 350\n",
            "running reward: 12.99 at episode 360\n",
            "running reward: 12.04 at episode 370\n",
            "running reward: 13.27 at episode 380\n",
            "running reward: 15.83 at episode 390\n",
            "running reward: 21.26 at episode 400\n",
            "running reward: 29.45 at episode 410\n",
            "running reward: 38.49 at episode 420\n",
            "running reward: 41.65 at episode 430\n",
            "running reward: 50.01 at episode 440\n",
            "running reward: 69.77 at episode 450\n",
            "running reward: 46.56 at episode 460\n",
            "running reward: 33.69 at episode 470\n",
            "running reward: 24.54 at episode 480\n",
            "running reward: 21.18 at episode 490\n",
            "running reward: 17.46 at episode 500\n",
            "running reward: 15.87 at episode 510\n",
            "running reward: 14.48 at episode 520\n",
            "running reward: 13.32 at episode 530\n",
            "running reward: 13.55 at episode 540\n",
            "running reward: 12.66 at episode 550\n",
            "running reward: 12.34 at episode 560\n",
            "running reward: 12.47 at episode 570\n",
            "running reward: 12.04 at episode 580\n",
            "running reward: 12.12 at episode 590\n",
            "running reward: 11.79 at episode 600\n",
            "running reward: 12.39 at episode 610\n",
            "running reward: 12.72 at episode 620\n",
            "running reward: 12.29 at episode 630\n",
            "running reward: 13.83 at episode 640\n",
            "running reward: 13.63 at episode 650\n",
            "running reward: 12.96 at episode 660\n",
            "running reward: 15.94 at episode 670\n",
            "running reward: 14.84 at episode 680\n",
            "running reward: 13.77 at episode 690\n",
            "running reward: 12.56 at episode 700\n",
            "running reward: 11.78 at episode 710\n",
            "running reward: 11.58 at episode 720\n",
            "running reward: 11.74 at episode 730\n",
            "running reward: 12.17 at episode 740\n",
            "running reward: 13.81 at episode 750\n",
            "running reward: 13.91 at episode 760\n",
            "running reward: 16.65 at episode 770\n",
            "running reward: 20.37 at episode 780\n",
            "running reward: 20.75 at episode 790\n",
            "running reward: 20.13 at episode 800\n",
            "running reward: 20.04 at episode 810\n",
            "running reward: 26.58 at episode 820\n",
            "running reward: 22.69 at episode 830\n",
            "running reward: 28.00 at episode 840\n",
            "running reward: 24.31 at episode 850\n",
            "running reward: 20.99 at episode 860\n",
            "running reward: 19.67 at episode 870\n",
            "running reward: 23.53 at episode 880\n",
            "running reward: 20.72 at episode 890\n",
            "running reward: 22.11 at episode 900\n",
            "running reward: 23.01 at episode 910\n",
            "running reward: 24.92 at episode 920\n",
            "running reward: 28.64 at episode 930\n",
            "running reward: 29.70 at episode 940\n",
            "running reward: 32.34 at episode 950\n",
            "running reward: 35.24 at episode 960\n",
            "running reward: 36.13 at episode 970\n",
            "running reward: 34.55 at episode 980\n",
            "running reward: 37.85 at episode 990\n",
            "running reward: 37.50 at episode 1000\n",
            "running reward: 39.15 at episode 1010\n",
            "running reward: 39.35 at episode 1020\n",
            "running reward: 36.41 at episode 1030\n",
            "running reward: 37.86 at episode 1040\n",
            "running reward: 37.75 at episode 1050\n",
            "running reward: 37.62 at episode 1060\n",
            "running reward: 37.63 at episode 1070\n",
            "running reward: 37.06 at episode 1080\n",
            "running reward: 39.46 at episode 1090\n",
            "running reward: 42.81 at episode 1100\n",
            "running reward: 45.07 at episode 1110\n",
            "running reward: 44.26 at episode 1120\n",
            "running reward: 42.88 at episode 1130\n",
            "running reward: 43.14 at episode 1140\n",
            "running reward: 48.27 at episode 1150\n",
            "running reward: 45.54 at episode 1160\n",
            "running reward: 46.40 at episode 1170\n",
            "running reward: 49.57 at episode 1180\n",
            "running reward: 49.78 at episode 1190\n",
            "running reward: 49.31 at episode 1200\n",
            "running reward: 52.59 at episode 1210\n",
            "running reward: 52.81 at episode 1220\n",
            "running reward: 54.60 at episode 1230\n",
            "running reward: 55.91 at episode 1240\n",
            "running reward: 57.29 at episode 1250\n",
            "running reward: 56.12 at episode 1260\n",
            "running reward: 56.23 at episode 1270\n",
            "running reward: 59.67 at episode 1280\n",
            "running reward: 61.56 at episode 1290\n",
            "running reward: 61.56 at episode 1300\n",
            "running reward: 60.38 at episode 1310\n",
            "running reward: 65.41 at episode 1320\n",
            "running reward: 65.04 at episode 1330\n",
            "running reward: 63.68 at episode 1340\n",
            "running reward: 64.24 at episode 1350\n",
            "running reward: 65.10 at episode 1360\n",
            "running reward: 63.39 at episode 1370\n",
            "running reward: 64.48 at episode 1380\n",
            "running reward: 69.78 at episode 1390\n",
            "running reward: 75.05 at episode 1400\n",
            "running reward: 71.90 at episode 1410\n",
            "running reward: 76.84 at episode 1420\n",
            "running reward: 74.09 at episode 1430\n",
            "running reward: 73.49 at episode 1440\n",
            "running reward: 74.29 at episode 1450\n",
            "running reward: 79.38 at episode 1460\n",
            "running reward: 74.76 at episode 1470\n",
            "running reward: 72.53 at episode 1480\n",
            "running reward: 77.76 at episode 1490\n",
            "running reward: 80.70 at episode 1500\n",
            "running reward: 80.72 at episode 1510\n",
            "running reward: 83.00 at episode 1520\n",
            "running reward: 85.00 at episode 1530\n",
            "running reward: 86.86 at episode 1540\n",
            "running reward: 93.22 at episode 1550\n",
            "running reward: 98.81 at episode 1560\n",
            "running reward: 95.76 at episode 1570\n",
            "running reward: 108.37 at episode 1580\n",
            "running reward: 100.52 at episode 1590\n",
            "running reward: 104.92 at episode 1600\n",
            "running reward: 95.84 at episode 1610\n",
            "running reward: 99.69 at episode 1620\n",
            "running reward: 100.64 at episode 1630\n",
            "running reward: 98.24 at episode 1640\n",
            "running reward: 101.44 at episode 1650\n",
            "running reward: 103.45 at episode 1660\n",
            "running reward: 105.96 at episode 1670\n",
            "running reward: 112.77 at episode 1680\n",
            "running reward: 120.70 at episode 1690\n",
            "running reward: 130.00 at episode 1700\n",
            "running reward: 147.28 at episode 1710\n",
            "running reward: 165.99 at episode 1720\n",
            "running reward: 179.19 at episode 1730\n",
            "running reward: 187.39 at episode 1740\n",
            "running reward: 188.89 at episode 1750\n",
            "running reward: 192.77 at episode 1760\n",
            "running reward: 195.24 at episode 1770\n",
            "Solved at episode 1770!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuitions\n",
        "\n",
        "### How does the actor learn?\n",
        "\n",
        "The overall purpose of actor network is easy to understand - it needs to pick the best possible actions so that the agent gets high rewards over time.\n",
        "\n",
        "But how does the actor loss value help with that? The loss value is calculated as:\n",
        "\n",
        "```\n",
        "actor_loss = -log_prob * (return - critic_value)\n",
        "```\n",
        "\n",
        "Let's focus on the `return - critic_value` first. The reason we have the critic's estimate is to figure out how good or bad the current state of the environment is. Not every situation inside an environment is one where there is an action that will result in high rewards. Maybe the agent is in a really bad situation, where they will get negative rewards no matter what action they choose. A critic will judge the situation and output an estimate of the rewards they can get in the future. And the agent should be tuned based on the difference of the actual rewards they ended up getting and the critic's estimate.\n",
        "\n",
        "The `logprob` represents the probability that the actor assigned to chosen action. Now, if the action led to good rewards, the difference between actual rewards and critic's estimate will be high. If the probability predicted for this action was high, `logprob` will be close to 0. Therefore, the product will be close to 0. If the probability was low, the loss will be high.\n",
        "\n",
        "If the action led to bad rewards, the difference between the actual rewards and the critic's estimate will be negative. If the probability for this bad action was low, the loss value will be high.\n",
        "\n",
        "So tuning the parameters in a way that brings this loss down will help the agent learn the correct behavior.\n",
        "\n",
        "### How does the critic learn?\n",
        "\n",
        "Critic is more straightforward. At the end of the episode, we know the total rewards the agent ended up getting at each timestep. So, we can simply use a loss function based on the difference between the critic's predicted estimate and the actual rewards. We use huber loss here.\n",
        "\n",
        "The rewards in the later timesteps are discounted to reflect the fact that the rewards which immediately follow the current state are more important than the rewards in the far future.\n",
        "\n",
        "Let me spell that out in a better way:\n",
        "\n",
        "At timestep `t`: The sum of rewards from this point onwards will be `r(t) + r(t+1) + ...`. We discount the future rewards by modifying this sum to:\n",
        "```\n",
        "r(t) + gamma * r(t+1) + gamma * gamma * r(t+2) + ...\n",
        "```\n",
        "where `gamma` is something like 0.99\n",
        "\n"
      ],
      "metadata": {
        "id": "_XGSmBTe1akj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank you\n",
        "\n",
        "Hit me up on [X](https://x.com/_apoorvnandan) if you any questions or feedback!\n",
        "\n",
        "[Buy me a coffee](https://buymeacoffee.com/apoorvn) if you would like to support my work!"
      ],
      "metadata": {
        "id": "7Qll5_trJD3o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuAQga2gFoJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BUrWRwWFoHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}